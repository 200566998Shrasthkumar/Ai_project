{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2JZt1GL5D6W"
   },
   "source": [
    "# **Introduction to CUDA and PyCUDA**\n",
    "\n",
    "PyCUDA gives you easy, Pythonic access to Nvidia’s CUDA parallel computation API. \n",
    "\n",
    "*   Abstractions make CUDA programming easier\n",
    "*   Full power of CUDA API if needed\n",
    "*   Automatic error checking and cleanup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rakk0eUMHRR7"
   },
   "source": [
    "## **Initialization**\n",
    "A few modules have to be loaded to initialize communication to the GPU:\n",
    "\n",
    "*   import pycuda.driver as cuda\n",
    "*   import pycuda.autoinit\n",
    "\n",
    "The pycuda.driver module has methods that:\n",
    "1. Allocate memory on the GPU (cuda.mem alloc())\n",
    "2. Copy numpy arrays to the GPU (cuda.memcpy htod())\n",
    "3. Execute kernels on the GPU described by CUDA code (see compiler.SourceModule)\n",
    "4. Copy data from the GPU back to numpy arrays (cuda.memcpy dtoh())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FA_YN7HlGRP5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: pip: not found\n"
     ]
    }
   ],
   "source": [
    "# Import PyCUDA and several modules associated with the PyCUDA\n",
    "!pip install pycuda \n",
    "import pycuda\n",
    "import pycuda.driver as cuda\n",
    "cuda.init()\n",
    "\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import pycuda.gpuarray as gpuarray\n",
    "from pycuda.curandom import rand as curand\n",
    "from pycuda.elementwise import ElementwiseKernel\n",
    "\n",
    "import numpy as np\n",
    "import numpy.linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7uA7OVrTLIl"
   },
   "source": [
    "## **CUDA device query**\n",
    "\n",
    "A GPU query is a very basic operation that will tell us the specific technical details of our GPU, such as available GPU memory and core count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TtnIrmDaQmm9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device query (PyCUDA version) \n",
      "\n",
      "Detected 1 CUDA Capable device(s) \n",
      "\n",
      "Device 0: NVIDIA Tegra X1\n",
      "\t Compute Capability: 5.3\n",
      "\t Total Memory: 3962 megabytes\n"
     ]
    }
   ],
   "source": [
    "print('CUDA device query (PyCUDA version) \\n')\n",
    "print('Detected {} CUDA Capable device(s) \\n'.format(cuda.Device.count()))\n",
    "for i in range(cuda.Device.count()):\n",
    "    \n",
    "    gpu_device = cuda.Device(i)\n",
    "    print('Device {}: {}'.format( i, gpu_device.name() ) )\n",
    "    compute_capability = float( '%d.%d' % gpu_device.compute_capability() )\n",
    "    print('\\t Compute Capability: {}'.format(compute_capability))\n",
    "    print('\\t Total Memory: {} megabytes'.format(gpu_device.total_memory()//(1024**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cE8Wg9b84Zvi"
   },
   "source": [
    "## **Basics of GPU programming**\n",
    "\n",
    "We'll see how to write and read data to and from the GPU's memory, and how to write some very simple elementwise GPU functions in CUDA C.\n",
    "\n",
    "### PyCUDA's gpuarray class\n",
    "\n",
    "PyCUDA's gpuarray class has important role within GPU programming in Python. This has all of the features from NumPy—multidimensional vector/matrix/tensor shape structuring, array-slicing, array unraveling, and overloaded operators for point-wise computations (for example, +, -, *, /, and **).\n",
    "\n",
    "### Transfering data to and from the GPU with gpuarray\n",
    "\n",
    "GPU has its own memory apart from the host computer's memory, which is known as device memory. \n",
    "In CUDA C, data is transfferd back and forth between the CPU to the GPU (with commands such as cudaMemcpyHostToDevice and cudaMemcpyDeviceToHost).\n",
    "\n",
    "Fortunately, PyCUDA covers all of the overhead of memory allocation, deallocation, and data transfers with the gpuarray class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "s0h99JVJn6T2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.  8. 10.]\n"
     ]
    }
   ],
   "source": [
    "#contain the host data\n",
    "host_data = np.array([1,2,3,4,5], dtype=np.float32)\n",
    "#transfer the host data to the GPU and create a new GPU array\n",
    "device_data = gpuarray.to_gpu(host_data)\n",
    "#perform pointwise multiplication on the GPU\n",
    "device_data_x2 = 2* device_data\n",
    "#retrieve the GPU data into a new with the gpuarray.get function\n",
    "host_data_x2 = device_data_x2.get()\n",
    "print(host_data_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU2DYoiMXP9J"
   },
   "source": [
    "## **Example: Doubling the value of elements in an array**\n",
    "\n",
    "Here, we will take an array and double the element of it on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHd1cmag_Wbx"
   },
   "source": [
    "### Step1: Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-g92eSBn5FlC"
   },
   "outputs": [],
   "source": [
    "# Declare the array as follows:\n",
    "np.random.seed(1729)\n",
    "a = np.random.randn(4,4).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AiqHKSh_ZdU"
   },
   "source": [
    "### Step 2: Transferring Data to the GPU\n",
    "\n",
    "The next step in most programs is to transfer data onto the device. In PyCuda, you will mostly transfer data from numpy arrays on the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "92L9E7WkJhsy"
   },
   "outputs": [],
   "source": [
    "# firstly, we need to allocate memory on the device\n",
    "a_gpu = cuda.mem_alloc(a.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "b1xaAt_NJjSb"
   },
   "outputs": [],
   "source": [
    "# we need to transfer the data to the GPU\n",
    "cuda.memcpy_htod(a_gpu, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGxPkOSsARmP"
   },
   "source": [
    "### Step 3: Executing a Kernel\n",
    "\n",
    "For this tutorial, we will write code to double each entry in a_gpu. To this end, we write the corresponding CUDA C code, and feed it into the constructor of a [pycuda.compiler.SourceModule](https://documen.tician.de/pycuda/driver.html#pycuda.compiler.SourceModule):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bSAkS6e2Jk38"
   },
   "outputs": [],
   "source": [
    "# Double each entry in the variable a_gpu\n",
    "mod = SourceModule(\"\"\"\n",
    "  __global__ void twice(float *a)\n",
    "  {\n",
    "    int idx = threadIdx.x + threadIdx.y*4;\n",
    "    a[idx] *= 2;\n",
    "  }\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4NQf5XcAsEI"
   },
   "source": [
    "If there aren’t any errors, the code is now compiled and loaded onto the device. We find a reference to [pycuda.driver.Function](https://documen.tician.de/pycuda/driver.html#pycuda.driver.Function) and call it, specifying a_gpu as the argument, and a block size of 4x4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fBQ0blkNJnIg"
   },
   "outputs": [],
   "source": [
    "func = mod.get_function(\"twice\")\n",
    "func(a_gpu, block=(4,4,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whSsByHBUaiY"
   },
   "source": [
    "Finally, we fetch the data back from the GPU and display it, together with the original a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_ef82NDPJqWV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [[-0.6873394  -0.82099473  1.6523609  -0.57529306]\n",
      " [ 1.0989678   0.92594606 -0.9934138  -0.8582211 ]\n",
      " [ 0.07488676  0.5293555   0.12095155 -0.22442362]\n",
      " [-1.5566785   0.05594088  0.16147153 -2.1346416 ]]\n",
      "\n",
      "Doubled value [[-1.3746789  -1.6419895   3.3047218  -1.1505861 ]\n",
      " [ 2.1979356   1.8518921  -1.9868276  -1.7164422 ]\n",
      " [ 0.14977352  1.058711    0.2419031  -0.44884723]\n",
      " [-3.113357    0.11188176  0.32294306 -4.2692833 ]]\n"
     ]
    }
   ],
   "source": [
    "a_doubled = np.empty_like(a)\n",
    "cuda.memcpy_dtoh(a_doubled, a_gpu)\n",
    "\n",
    "print(\"a\",a)\n",
    "print(\"\\nDoubled value\",a_doubled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHBgezKxBdnE"
   },
   "source": [
    "### Abstracting Away the Complications\n",
    "\n",
    "Using a [pycuda.gpuarray.GPUArray](https://documen.tician.de/pycuda/array.html#pycuda.gpuarray.GPUArray), the same effect can be achieved with much less writing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "24Wk5V33eLUQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [[-0.6873394  -0.82099473  1.6523609  -0.57529306]\n",
      " [ 1.0989678   0.92594606 -0.9934138  -0.8582211 ]\n",
      " [ 0.07488676  0.5293555   0.12095155 -0.22442362]\n",
      " [-1.5566785   0.05594088  0.16147153 -2.1346416 ]]\n",
      "\n",
      "Doubled value [[-1.3746789  -1.6419895   3.3047218  -1.1505861 ]\n",
      " [ 2.1979356   1.8518921  -1.9868276  -1.7164422 ]\n",
      " [ 0.14977352  1.058711    0.2419031  -0.44884723]\n",
      " [-3.113357    0.11188176  0.32294306 -4.2692833 ]]\n"
     ]
    }
   ],
   "source": [
    "# implementing with gpu_array\n",
    "a_gpu = gpuarray.to_gpu(a)\n",
    "a_doubled = (2*a_gpu).get()\n",
    "print(\"a\",a)\n",
    "print(\"\\nDoubled value\",a_doubled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Kd13_mbB8eb"
   },
   "source": [
    "### Shortcuts for Explicit Memory Copies\n",
    "\n",
    "The [pycuda.driver.In](https://documen.tician.de/pycuda/driver.html#pycuda.driver.In), [pycuda.driver.Out](https://documen.tician.de/pycuda/driver.html#pycuda.driver.Out), and [pycuda.driver.InOut](https://documen.tician.de/pycuda/driver.html#pycuda.driver.InOut) argument handlers can simplify some of the memory transfers. For example, instead of creating a_gpu, if replacing a is fine, the following code can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "f6-BYWwsniNR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [[-1.3746789  -1.6419895   3.3047218  -1.1505861 ]\n",
      " [ 2.1979356   1.8518921  -1.9868276  -1.7164422 ]\n",
      " [ 0.14977352  1.058711    0.2419031  -0.44884723]\n",
      " [-3.113357    0.11188176  0.32294306 -4.2692833 ]]\n"
     ]
    }
   ],
   "source": [
    "# implementing with InOut\n",
    "func(cuda.InOut(a), block=(4, 4, 1))\n",
    "print(\"a\",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhIg22xHYJU9"
   },
   "source": [
    "## **Example: Addition of two 1D arrays**\n",
    "\n",
    "Here, we will add two 1D arrays and execute it on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hHZsceK_Zlxr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m [ 0.10967004  0.44301215  0.39626622  0.2497974   1.2984973  -1.2804337\n",
      " -0.97546583 -0.26908663 -1.1057384  -0.1279927 ]\n",
      "\n",
      "n [-0.61782736 -0.98912627 -2.8598924  -0.7943475  -0.30579695  1.7006376\n",
      " -0.63544416  0.00457387  1.133304    0.14164127]\n",
      "\n",
      "sum [-0.5081573  -0.5461141  -2.4636261  -0.5445501   0.99270034  0.42020392\n",
      " -1.6109099  -0.26451278  0.0275656   0.01364857]\n"
     ]
    }
   ],
   "source": [
    "# declare arrays\n",
    "m = np.random.randn(10).astype(np.float32)\n",
    "n = np.random.randn(10).astype(np.float32)\n",
    "\n",
    "# execute the kernel\n",
    "mod2_1D = SourceModule(\"\"\"\n",
    "__global__ void sum2arr(float *sum, float *m, float *n)\n",
    "{\n",
    "  const int i = threadIdx.x;\n",
    "  sum[i] = m[i] + n[i];\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "func = mod2_1D.get_function(\"sum2arr\")\n",
    "\n",
    "# handle memory transfer with 'Out()'\n",
    "sum_1D = np.zeros_like(m)\n",
    "func(cuda.Out(sum_1D),\n",
    "     cuda.In(m), cuda.In(n),\n",
    "     block=(10,1,1))\n",
    "\n",
    "# print result\n",
    "print(\"m\",m)\n",
    "print(\"\\nn\",n)\n",
    "print(\"\\nsum\",sum_1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Si7HbO0gbjTQ"
   },
   "source": [
    "## **Example: Addition of matrices**\n",
    "\n",
    "Here, we will add two matrices and execute it on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kXx_p97mJs4Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b [[ 2.091718   -1.2695593  -0.7228234  -0.01938889]\n",
      " [-0.18282357 -1.1190658  -0.6982751   2.1070845 ]\n",
      " [-0.0025556   0.40901005  0.28363675 -1.9498545 ]\n",
      " [ 0.38603088  0.23064111 -1.1869708  -0.6665516 ]]\n",
      "\n",
      "c [[ 1.7977669   0.66379833  1.1238116  -1.1343062 ]\n",
      " [ 1.1844498   0.70126235  0.45549637 -0.0814869 ]\n",
      " [-1.0958686   0.4407169   1.6427085   0.4479594 ]\n",
      " [-0.7867167  -0.17314298  2.319482   -0.32860455]]\n",
      "\n",
      "sum [[ 3.889485   -0.60576093  0.40098822 -1.1536951 ]\n",
      " [ 1.0016263  -0.4178034  -0.24277872  2.0255976 ]\n",
      " [-1.0984242   0.8497269   1.9263453  -1.5018951 ]\n",
      " [-0.40068582  0.05749813  1.1325113  -0.99515617]]\n"
     ]
    }
   ],
   "source": [
    "# declare matrices\n",
    "b = np.random.randn(4,4).astype(np.float32)\n",
    "c = np.random.randn(4,4).astype(np.float32)\n",
    "\n",
    "# execute the kernel\n",
    "mod2_2D = SourceModule(\"\"\"\n",
    "  __global__ void add2(float *a, float *b)\n",
    "  {\n",
    "    int idx = threadIdx.x + threadIdx.y*4;\n",
    "    a[idx] += b[idx];\n",
    "  }\n",
    "  \"\"\")\n",
    "\n",
    "func = mod2_2D.get_function(\"add2\")\n",
    "\n",
    "# transfer the data to the GPU\n",
    "b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "c_gpu = cuda.mem_alloc(c.nbytes)\n",
    "\n",
    "cuda.memcpy_htod(b_gpu, b)\n",
    "cuda.memcpy_htod(c_gpu, c)\n",
    "\n",
    "func(b_gpu,c_gpu, block=(4,4,1))\n",
    "\n",
    "sum_2D = np.empty_like(b)\n",
    "cuda.memcpy_dtoh(sum_2D, b_gpu)\n",
    "\n",
    "# print result\n",
    "print(\"b\",b)\n",
    "print(\"\\nc\",c)\n",
    "print(\"\\nsum\",sum_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGGhkm8xY-cr"
   },
   "source": [
    "##**Example: Multiplication of matrices**\n",
    "\n",
    "Here, we will multiple two matrices and execute it on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dAKCn25Vggp"
   },
   "outputs": [],
   "source": [
    "# declare matrices\n",
    "r = np.random.randn(10).astype(np.float32)\n",
    "s = np.random.randn(10).astype(np.float32)\n",
    "\n",
    "# execute kernel\n",
    "mod3 = SourceModule(\"\"\"\n",
    "__global__ void multiply2arr(float *dest, float *r, float *s)\n",
    "{\n",
    "  const int i = threadIdx.x;\n",
    "  dest[i] = r[i] * s[i];\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "func = mod3.get_function(\"multiply2arr\")\n",
    "\n",
    "product = np.zeros_like(r)\n",
    "\n",
    "# handle memory transfer\n",
    "func(cuda.Out(product),\n",
    "     cuda.In(r), cuda.In(s),\n",
    "     block=(10,1,1))\n",
    "\n",
    "# print result\n",
    "print(\"r\",r)\n",
    "print(\"\\ns\",s)\n",
    "print(\"\\nproduct\",product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d69Myu1ksP-"
   },
   "source": [
    "## **Example: Linear combination of variables**\n",
    "\n",
    "The functionality in the module [pycuda.elementwise](https://documen.tician.de/pycuda/array.html#module-pycuda.elementwise) contains tools to help generate kernels that evaluate multi-stage expressions on one or several operands in a single pass. Here's a usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zmqyWfJpjw7X"
   },
   "outputs": [],
   "source": [
    "# generate arrays of random numbers using curand()\n",
    "n1_gpu = curand((10,))\n",
    "n2_gpu = curand((10,))\n",
    "\n",
    "# generate a kernel that takes a number of scalar or vector arguments and performs the scalar operation on each entry of its arguments, if that argument is a vector.\n",
    "linear_combination = ElementwiseKernel(\n",
    "        \"float a, float *x, float b, float *y, float *z\",\n",
    "        \"z[i] = my_f(a*x[i], b*y[i])\",\n",
    "        \"linear_combination\",\n",
    "        preamble=\"\"\"\n",
    "        __device__ float my_f(float x, float y)\n",
    "        { \n",
    "          return sin(x*y);\n",
    "        }\n",
    "        \"\"\")\n",
    "\n",
    "# make a new, uninitialized GPUArray having the same properties as other_ary\n",
    "c_gpu = gpuarray.empty_like(n1_gpu)\n",
    "\n",
    "# call the function\n",
    "linear_combination(5, n1_gpu, 6, n2_gpu, c_gpu)\n",
    "\n",
    "# test for a particular condition\n",
    "assert la.norm(c_gpu.get() - np.sin((5*n1_gpu*6*n2_gpu).get())) < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3y8KJLfTmTGo"
   },
   "source": [
    "## **Example: Numba and PyCUDA**\n",
    "\n",
    "The module [pycuda.autoprimaryctx](https://documen.tician.de/pycuda/util.html#module-pycuda.autoprimaryctx) is similar to [pycuda.autoinit](https://documen.tician.de/pycuda/util.html#module-pycuda.autoinit), except that it retains the device primary context instead of creating a new context in [pycuda.tools.make_default_context()](https://documen.tician.de/pycuda/util.html#pycuda.tools.make_default_context). Notably, it also has device and context attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UDtaDlzzmWWv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [[-0.6873394  -0.82099473  1.6523609  -0.57529306]\n",
      " [ 1.0989678   0.92594606 -0.9934138  -0.8582211 ]\n",
      " [ 0.07488676  0.5293555   0.12095155 -0.22442362]\n",
      " [-1.5566785   0.05594088  0.16147153 -2.1346416 ]]\n",
      "Doubling values using numba:  [[-1.3746789  -1.6419895   3.3047218  -1.1505861 ]\n",
      " [ 2.1979356   1.8518921  -1.9868276  -1.7164422 ]\n",
      " [ 0.14977352  1.058711    0.2419031  -0.44884723]\n",
      " [-3.113357    0.11188176  0.32294306 -4.2692833 ]]\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "\n",
    "# Using autoprimaryctx instead of autoinit since Numba can only operate on a primary context\n",
    "import pycuda.autoprimaryctx  \n",
    "\n",
    "# Creating a PyCUDA gpuarray\n",
    "print(\"a\",a_gpu)\n",
    "\n",
    "# Numba kernel\n",
    "@cuda.jit\n",
    "def double(x):\n",
    "    i, j = cuda.grid(2)\n",
    "\n",
    "    if i < x.shape[0] and j < x.shape[1]:\n",
    "        x[i, j] *= 2\n",
    "\n",
    "# Calling the Numba kernel on the PyCUDA gpuarray, using the CUDA Array Interface transparently\n",
    "double[(4, 4), (1, 1)](a_gpu)\n",
    "print(\"Doubling values using numba: \",a_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjdE0VJJPNna"
   },
   "source": [
    "# **Exercise**\n",
    "\n",
    "- Write a cuda kernel to find the elementwise square of a matrix\n",
    "- Write a cuda kernel to find a matrix, which when added to the given matrix results in every element being equal to zero\n",
    "- Write a cuda kernel to multiply two matrices:\n",
    "  - Assume square matrices, with dimensions < 1024\n",
    "  - Assume square matrices, with dimensions > 1024\n",
    "  - Assume non-square matrices, with dimensions > 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "B14RsBKfPO7v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01937819 0.42511994 0.33997074 0.63602585]\n",
      " [0.03652494 0.4406518  0.8636695  0.76425326]\n",
      " [0.03750517 0.09043005 0.06292532 0.02009321]\n",
      " [0.25513023 0.20018847 0.63525164 0.25055218]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def square_matrix(A):\n",
    "    row, col = cuda.grid(2)\n",
    "    N = A.shape[0]  # Assuming square matrix\n",
    "\n",
    "    if row < N and col < N:\n",
    "        A[row, col] *= A[row, col]  # Square each element\n",
    "\n",
    "# Define matrix size\n",
    "N = 4  # Example size\n",
    "A = np.random.rand(N, N).astype(np.float32)  # Random matrix\n",
    "\n",
    "# Copy to device\n",
    "d_A = cuda.to_device(A)\n",
    "\n",
    "# Define thread and block configuration\n",
    "threads_per_block = (16, 16)\n",
    "blocks_per_grid = ((N + threads_per_block[0] - 1) // threads_per_block[0],\n",
    "                   (N + threads_per_block[1] - 1) // threads_per_block[1])\n",
    "\n",
    "# Launch kernel\n",
    "square_matrix[blocks_per_grid, threads_per_block](d_A)\n",
    "\n",
    "# Copy result back to host\n",
    "A_squared = d_A.copy_to_host()\n",
    "print(A_squared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix A:\n",
      " [[0.2995714  0.99545646 0.7228736  0.3163466 ]\n",
      " [0.52194184 0.71298903 0.54312956 0.6573724 ]\n",
      " [0.05103904 0.9879216  0.530643   0.27212656]\n",
      " [0.16180052 0.9762489  0.7255236  0.04226469]]\n",
      "Negated Matrix B:\n",
      " [[-0.2995714  -0.99545646 -0.7228736  -0.3163466 ]\n",
      " [-0.52194184 -0.71298903 -0.54312956 -0.6573724 ]\n",
      " [-0.05103904 -0.9879216  -0.530643   -0.27212656]\n",
      " [-0.16180052 -0.9762489  -0.7255236  -0.04226469]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "# Define CUDA kernel code as a string\n",
    "kernel_code = \"\"\"\n",
    "__global__ void negateMatrix(float *A, float *B, int N) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < N && col < N) {\n",
    "        int index = row * N + col;\n",
    "        B[index] = -A[index];  // Compute negative\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Define matrix size\n",
    "N = 4  # Example size\n",
    "A = np.random.rand(N, N).astype(np.float32)  # Random matrix\n",
    "B = np.zeros_like(A)  # Matrix to store the negated result\n",
    "\n",
    "# Allocate memory on the device\n",
    "A_gpu = cuda.mem_alloc(A.nbytes)\n",
    "B_gpu = cuda.mem_alloc(B.nbytes)\n",
    "\n",
    "# Copy data to device\n",
    "cuda.memcpy_htod(A_gpu, A)\n",
    "\n",
    "# Compile the kernel\n",
    "mod = SourceModule(kernel_code)\n",
    "\n",
    "# Get the kernel function\n",
    "negate_matrix = mod.get_function(\"negateMatrix\")\n",
    "\n",
    "# Define thread and block configuration\n",
    "threads_per_block = (16, 16, 1)\n",
    "blocks_per_grid = ((N + threads_per_block[0] - 1) // threads_per_block[0],\n",
    "                   (N + threads_per_block[1] - 1) // threads_per_block[1])\n",
    "\n",
    "# Launch the kernel\n",
    "negate_matrix(A_gpu, B_gpu, np.int32(N), block=threads_per_block, grid=blocks_per_grid)\n",
    "\n",
    "# Copy result back to host\n",
    "cuda.memcpy_dtoh(B, B_gpu)\n",
    "\n",
    "print(\"Original Matrix A:\\n\", A)\n",
    "print(\"Negated Matrix B:\\n\", B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting Matrix C:\n",
      " [[1020.9612  1013.0582  1019.6845  ... 1017.9617  1027.5701   995.62524]\n",
      " [1021.8103  1011.1824  1013.0184  ... 1022.8366  1022.0986  1004.51575]\n",
      " [1021.0074  1004.4751  1007.10974 ... 1017.53357 1024.4464  1006.3561 ]\n",
      " ...\n",
      " [1040.8395  1021.2631  1030.9387  ... 1036.1636  1038.9098  1024.4077 ]\n",
      " [1028.2521  1014.87695 1029.2435  ... 1037.9235  1023.98456 1019.856  ]\n",
      " [1052.5536  1041.3102  1046.9324  ... 1058.4493  1065.2323  1045.659  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "# Define CUDA kernel code for matrix multiplication\n",
    "kernel_code = \"\"\"\n",
    "__global__ void matrix_multiply(float *A, float *B, float *C, int N, int M, int K) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < N && col < K) {\n",
    "        float value = 0.0;\n",
    "        for (int i = 0; i < M; i++) {\n",
    "            value += A[row * M + i] * B[i * K + col];  // Matrix multiplication\n",
    "        }\n",
    "        C[row * K + col] = value;  // Store result in matrix C\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Matrix dimensions\n",
    "N = 1024  # Row size of A and C (and square matrices for case 1)\n",
    "M = 1024  # Column size of A and row size of B (and square matrices for case 2)\n",
    "K = 1024  # Column size of B (and square matrices for case 3)\n",
    "\n",
    "# For square matrices with dimensions < 1024, N, M, K < 1024\n",
    "# A = np.random.rand(N, M).astype(np.float32)\n",
    "# B = np.random.rand(M, K).astype(np.float32)\n",
    "# C = np.zeros((N, K), dtype=np.float32)\n",
    "\n",
    "# For non-square matrices with dimensions > 1024, using larger N, M, K\n",
    "N_large = 2048  # Example for non-square matrix A\n",
    "M_large = 4096  # Example for non-square matrix B\n",
    "K_large = 2048  # Resulting matrix C\n",
    "\n",
    "A_large = np.random.rand(N_large, M_large).astype(np.float32)\n",
    "B_large = np.random.rand(M_large, K_large).astype(np.float32)\n",
    "C_large = np.zeros((N_large, K_large), dtype=np.float32)\n",
    "\n",
    "# Allocate memory on the device\n",
    "A_gpu = cuda.mem_alloc(A_large.nbytes)\n",
    "B_gpu = cuda.mem_alloc(B_large.nbytes)\n",
    "C_gpu = cuda.mem_alloc(C_large.nbytes)\n",
    "\n",
    "# Copy matrices to device\n",
    "cuda.memcpy_htod(A_gpu, A_large)\n",
    "cuda.memcpy_htod(B_gpu, B_large)\n",
    "\n",
    "# Compile the kernel\n",
    "mod = SourceModule(kernel_code)\n",
    "\n",
    "# Get the kernel function\n",
    "matrix_multiply = mod.get_function(\"matrix_multiply\")\n",
    "\n",
    "# Define thread and block configuration\n",
    "threads_per_block = (16, 16, 1)\n",
    "blocks_per_grid = ((K_large + threads_per_block[0] - 1) // threads_per_block[0],\n",
    "                   (N_large + threads_per_block[1] - 1) // threads_per_block[1])\n",
    "\n",
    "# Launch the kernel for matrix multiplication\n",
    "matrix_multiply(A_gpu, B_gpu, C_gpu, np.int32(N_large), np.int32(M_large), np.int32(K_large),\n",
    "                 block=threads_per_block, grid=blocks_per_grid)\n",
    "\n",
    "# Copy result back to host\n",
    "cuda.memcpy_dtoh(C_large, C_gpu)\n",
    "\n",
    "print(\"Resulting Matrix C:\\n\", C_large)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Rakk0eUMHRR7",
    "u7uA7OVrTLIl",
    "cE8Wg9b84Zvi",
    "JU2DYoiMXP9J",
    "ZHd1cmag_Wbx",
    "0AiqHKSh_ZdU",
    "nGxPkOSsARmP",
    "UHBgezKxBdnE",
    "1Kd13_mbB8eb",
    "rhIg22xHYJU9",
    "Si7HbO0gbjTQ",
    "oGGhkm8xY-cr",
    "8d69Myu1ksP-",
    "3y8KJLfTmTGo",
    "AjdE0VJJPNna"
   ],
   "name": "Module-1-Lab-cuda-pycuda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
